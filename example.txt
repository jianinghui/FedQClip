# Example Usage of the Federated Learning Script

This example demonstrates how to run the federated learning script with specific parameters.

### Parameters

- **num_clients:** Number of clients participating in the federated learning. (Default: 4)
- **num_rounds:** Number of communication rounds between clients and server. (Default: 100)
- **num_epochs_per_round:** Number of epochs each client trains per round. (Default: 5)
- **eta_c:** Learning rate for the clients. (Default: 0.05)
- **gamma_c:** Scaling factor for client updates. (Default: 10,this means Clipped SGD)
- **eta_s:** Learning rate for the server. (Default: 0.05)
- **gamma_s:** Scaling factor for server updates. (Default: 1000000, this means SGD)
- **quantize:** Whether to quantize the updates. (Default: True)
- **bit:** Number of bits for quantization. (Default: 4)
- **flag:** Whether to generate and save data distribution plots. (Default: True)
- **alpha:** Dirichlet distribution parameter to control non-IID data distribution. (Default: 1.0)
- **iid:** Whether to split data in an IID manner. (Default: False)
- **batch_size:** Batch size for training. (Default: 64)
- **model_name:** Model architecture to use (MobileNetV3, ResNet18). (Default: MobileNetV3)
- **dataset_name:** Dataset to use (CIFAR100, TinyImagenet). (Default: TinyImagenet)

### Example Command

To run the federated learning script with specific parameters, use the following command:

```sh
python federated_learning.py --num_clients 10 --num_rounds 50 --num_epochs_per_round 3 --eta_c 0.01 --gamma_c 10000 --eta_s 0.01 --gamma_s 10000 --quantize True --bit 8 --flag True --alpha 0.5 --iid False --batch_size 32 --model_name ResNet18 --dataset_name CIFAR10
